{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GFTkxHQUxC3"
   },
   "source": [
    "# Q2 Naive-Bayes Classification (30 Points)\n",
    "## Definition\n",
    "Naive Bayes is a relatively simple classification algorithm based on probability and uses Bayes Theorom with an independence assumption among the features in the data. The fundamental idea of Naive Bayes is that it computes the probability of every class, which we want to reveal, based on the probability of every feature in the data.\n",
    "\n",
    "According to Naive Bayes algorithm, we are going to assume that every feature in the data is in an independent condition on the outcome probability of each separate class. Let's assume that we are doing a car classification and we have a data such as;\n",
    "\n",
    "| buying   | maint    | doors    | persons  | lug-boot | safety   | class    |\n",
    "| :------- | :------- | :------- | :------- | :------- | :------- | :------- |\n",
    "| vvhigh   | vhigh    | 2        | 2        | small    | low      | unacc    |\n",
    "\n",
    "**Description of dataset:**\n",
    "* CAR                      car acceptability\n",
    "    * PRICE                  overall price\n",
    "        * _buying_               buying price\n",
    "        * _maint_                price of the maintenance\n",
    "* TECH                   technical characteristics\n",
    "    * COMFORT              comfort\n",
    "        * _doors_              number of doors\n",
    "        * _persons_            capacity in terms of persons to carry\n",
    "        * _lug-boot_           the size of luggage boot\n",
    "    * _safety_               estimated safety of the car\n",
    "   \n",
    "Naive Bayes assumes that above mentioned features are independent of each other.\n",
    "\n",
    "In machine learning, Naive Bayes is advantageous against other commonly used classification algorithms because of its simplicity, speed and accuracy on small datasets and it also enables us to make classification despite missing information. Naive Bayes is a supervised learning algorithm because it needs to be trained with a labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdEqZEjdUxC5"
   },
   "source": [
    "## Bayes Theorem\n",
    "Consider two events, $A$ and $B$. For example, $A$ is a set of car features, which are $A \\in \\{ vvhigh, vhigh, 2, 2, small, low \\}$,and $B$ is a set of car classes that are $B \\in \\{ unacc, acc, good, vgood \\}$\n",
    "\n",
    "\n",
    "* $A \\cap B$ means the intersection of $A$ and $B$.\n",
    "* $P(A \\mid B)$ is read as probability of A given B.\n",
    "\n",
    "When we know that $B$ is given (Event $B$ has occurred), it means our sample space is $B$ that is the right figure. Now we are trying to compute the probability of also occuring $A$ at the same time (the conditional probability of $A$). It is obvious that we are trying to find the probability of $A \\cap B$ given that we are in the space of $B$.\n",
    "\n",
    "\\begin{equation}\n",
    "P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "We can rewrite $P(A \\cap B)$ as $P(A, B)$. Two of these mean the probability of $A$ and $B$ at the same time. So the new form of the equation is :\n",
    "\n",
    "\\begin{equation}\n",
    "P(A \\mid B) = \\frac{P(A, B)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "For the probability of $A$ and $B$, we can deduce equations below from the figure above.\n",
    "\n",
    "\\begin{align}\n",
    "& P(A, B) = P(B, A) = P(A \\mid B)P(B) \\\\\n",
    "& P(A, B) = P(B, A) = P(B \\mid A)P(A)\n",
    "\\end{align}\n",
    "\n",
    "Let's look at the new form of the equation putting the second form of $P(A, B)$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "This equation is known as **Bayes Theorem**.\n",
    "* $P(A \\mid B)$ : posterior that is the probability of $A$ when it is known that $B$ is given\n",
    "* $P(B)$ : evidence that is the marginal probability of $B$\n",
    "* $P(B \\mid A)$ : likelihood\n",
    "* $P(A)$ : prior probability that is marginal probabiliy of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2aZO4EYUxC6"
   },
   "source": [
    "## Naive-Bayes Formulation\n",
    "Suppose we have a dataset which each observation belongs to a class from the finite set $C = \\{ c_1, c_2, ..., c_n \\}$ and each observation constitutes from a few features $F = \\{ f_1, f_2, ..., f_b \\}$. If we could compute the probabilities of $P(c_1 | F), P(c_2 | F), ..., P(c_n | F)$ then we could predict the class for a new observation $i$ to be one of those which have the highest probability.\n",
    "\n",
    "To compute the conditional probabilities, we can use Bayes Theorem;\n",
    "\n",
    "\\begin{equation}\n",
    "P(c_i \\mid f_1, f_2, \\dots ,f_b) = \\frac{P(f_1, f_2, \\dots ,f_b \\mid c_i)P(c_i)}{P(f_1, f_2, \\dots ,f_b)} \n",
    "\\end{equation}\n",
    "\n",
    "As you know, Naive-Bayes supposes that all features are in independent conditions, therefore we can rewrite this equation like;\n",
    "\n",
    "\\begin{equation}\n",
    "P(c_i \\mid f_1, f_2, \\dots ,f_b) = \\frac{P(f_1 \\mid c_i)P(f_2 \\mid c_i) \\dots P(f_b \\mid c_i)P(c_i)}{P(f_1, f_2, \\dots ,f_b)} \n",
    "\\end{equation}\n",
    "\n",
    "The final form of equation is\n",
    "\n",
    "\\begin{align}\n",
    "& \\text{for} \\; i = 1, 2, \\dots , n \\\\\n",
    "& P(c_i \\mid f_1, f_2, \\dots ,f_b) = P(c_i) \\frac{\\Pi_{j=1}^b P(f_j \\mid c_i)}{P(f_1, f_2, \\dots ,f_b)} \n",
    "\\end{align}\n",
    "\n",
    "Since $P(f_1, f_2, \\dots ,f_b)$ is a constant, we can use the classification rule below.\n",
    "\n",
    "\\begin{align}\n",
    "& P(c_i \\mid f_1, f_2, \\dots ,f_b) \\propto P(c_i) \\Pi_{j=1}^b P(f_j \\mid c_i)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task. \n",
    "- Use the 'car_eval.csv' data set. Train a Naive Bayes model  using odd-indexed rows. Test the accuracy of your model using even-indexed rows of the dataset.\n",
    "- Display and explain the likelihood (Class conditional probabilities) for each input variable.\n",
    "- Discuss one missclassified case for each category in terms of class conditional probabilities. Why do you think it was missclassified.\n",
    "\n"
   ],
   "metadata": {
    "id": "-3d8iwGJle-K"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# if dataset.index % test_indis == 0 \n",
    "# then it is going to be used as test dataset\n",
    "# they will not be appended into the train dataset\n",
    "\n",
    "dataset = pd.read_csv('car-eval.csv')\n",
    "test_indis = 2\n",
    "\n",
    "train_dataset = dataset[dataset.index % test_indis != 0]\n",
    "test_dataset = dataset[dataset.index % test_indis == 0]\n",
    "\n",
    "# total count of sample space\n",
    "total = len(train_dataset)"
   ],
   "metadata": {
    "id": "9S2PQvmcioOp"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### Your code here\n",
    "# Expected accuracy 81%"
   ],
   "metadata": {
    "id": "vQh1UM6DWOy2"
   },
   "execution_count": 6,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
